{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c2a5d3",
   "metadata": {},
   "source": [
    "# BigQuery Data Warehousing Script\n",
    "\n",
    "**Link To Data**: https://data.cityofnewyork.us/City-Government/NYC-Citywide-Annualized-Calendar-Sales-Update/w2pb-icbu\n",
    "\n",
    "**API end-point**: https://data.cityofnewyork.us/resource/w2pb-icbu.json\n",
    "\n",
    "**Data Dictionary**: https://data.cityofnewyork.us/api/views/w2pb-icbu/files/8ed811b4-8238-4b5e-9acc-1e33d8705498?download=true&filename=Annualized_Calendar_Sales_Update%20Data_Dictionary.xlsx\n",
    "\n",
    "**Cleaned Data Dictionary**: https://docs.google.com/spreadsheets/d/17XyGmnw2fZuTMCWVKB1XiWGHQuwqWOidm0w80lbIyjE/edit?usp=sharing\n",
    "\n",
    "**IMPORTANT: This data set is 121.3 MB. Once downloaded, please keep the file in the same directory as this jupyter notebook file, so that the .csv file can be uploaded to the Google Cloud correctly. This script will now allow us to upload our Data Model from our dataset in our Google Cloud Storage and into BigQuery**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c669cd7",
   "metadata": {},
   "source": [
    "# Data Model\n",
    "\n",
    "## Fact Table:\n",
    "\n",
    "**SALES_FACT**\n",
    "\n",
    "- SALE_ID (Primary Key, String)\n",
    "- DATE_ID (Foreign Key to DIM_DATE, Int)\n",
    "- LOCATION_ID (Foreign Key to DIM_LOCATION, Int)\n",
    "- SALE_PRICE (Int)\n",
    "- RESIDENTIAL_UNITS (Int)\n",
    "- COMMERCIAL_UNITS (Int)\n",
    "- TOTAL_UNITS (Int)\n",
    "- LAND_SQFT (Int)\n",
    "- GROSS_SQFT(Int)\n",
    "- INITIAL_BUILDING_CLASS (String)\n",
    "- FINAL_BUILDING_CLASS (String)\n",
    "- INITIAL_TAX_CLASS (Float)\n",
    "- FINAL_TAX_CLASS (Float)\n",
    "- PROPERTIES_UNSOLD (Boolean)\n",
    "- PROPERTIES_UNSOLD_PRE_2020 (Boolean)\n",
    "- PROPERTIES_UNSOLD_POST_2020 (Boolean)\n",
    "- PROPERTIES_SOLD (Boolean)\n",
    "- PROPERTIES_SOLD_POST_2020 (Boolean)\n",
    "- PROPERTIES_SOLD_PRE_2020 (Boolean)\n",
    "\n",
    "## Dimension Tables:\n",
    "\n",
    "**Date Dimension (DIM_DATE)**\n",
    "\n",
    "- DATE_ID (Primary Key, Int)\n",
    "- SALE_DATE (Date)\n",
    "- YEAR_BUILT (Int)\n",
    "- YEAR_SOLD (Int)\n",
    "- MONTH_SOLD (Int)\n",
    "- DAY_SOLD (Int)\n",
    "\n",
    "\n",
    "**Location Dimension (DIM_LOCATION)**\n",
    "\n",
    "- LOCATION_ID (Primary Key, Int)\n",
    "- BIN (Int)\n",
    "- ADDRESS (String)\n",
    "- BOROUGH (String)\n",
    "- NEIGHBORHOOD (String)\n",
    "- ZIP_CODE (Int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bdae7f",
   "metadata": {},
   "source": [
    "# Grant Required Permissions to your Google Cloud Service Account to Create a BigQuery Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d061580",
   "metadata": {},
   "source": [
    "**1. Grant Permissions:**\n",
    "- Go to the Google Cloud Console.\n",
    "- Navigate to the IAM & Admin > IAM page.\n",
    "- Locate the user account associated with the credentials you are using.\n",
    "- Click \"ADD IAM CONDITION\"\n",
    "- Under the \"Role\" field, select the \"BigQuery Data Editor\"\n",
    "- Click \"SAVE\" to grant permissions.\n",
    "\n",
    "**Note: The BigQuery Data Editor role allows your service account access to edit all the contents of datasets. This step is important for loading your dataset from Google Cloud to the BigQuery Data Warehouse.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb701df2",
   "metadata": {},
   "source": [
    "# Install the google-cloud-bigquery library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12118cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d426e43a",
   "metadata": {},
   "source": [
    "# Install pandas gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas gcsfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16deb34c",
   "metadata": {},
   "source": [
    "# Install pyarrow library\n",
    "\n",
    "**NOTE: Once pyarrow is installed, you should be able to use the load_table_from_dataframe function without encountering the ValueError from the \"Load Data into BigQuery Tables\" Cell.**\n",
    "\n",
    "**After installing pyarrow, you might need to restart your Python environment or Jupyter Notebook kernel before running the script again to ensure that the changes take effect.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b7202",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479a2e8f",
   "metadata": {},
   "source": [
    "# Import the Python 'os' module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a62f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'GOOGLE_CLOUD_ACCESSKEY.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773fb2ce",
   "metadata": {},
   "source": [
    "# Create BigQuery Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Creates a function for creating a BigQuery dataset with your file stored in your Google Cloud\n",
    "def create_bigquery_dataset(project_id, dataset_name):\n",
    "    bigquery_client = bigquery.Client(project=project_id)\n",
    "    dataset_id = f\"{project_id}.{dataset_name}\"\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "    dataset.location = \"US\"\n",
    "    bigquery_client.create_dataset(dataset)\n",
    "    print(f\"Dataset {dataset_id} created.\")\n",
    "\n",
    "# Replace \"your-project-id\" with your actual Google Cloud project id\n",
    "project_id = 'your-project-id'\n",
    "dataset_name = 'NYC_sales_cleaned'  \n",
    "create_bigquery_dataset(project_id, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794c0917",
   "metadata": {},
   "source": [
    "# Create Tables in BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Get the path to the service account key file from the environment variable\n",
    "service_account_path = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "\n",
    "# Set your Google Cloud credentials using the environment variable\n",
    "credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "# Initialize a BigQuery client\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "# Define your dataset and table names\n",
    "dataset_name = 'NYC_sales_cleaned'\n",
    "fact_table_name = 'SALES_FACT'\n",
    "date_dim_table_name = 'DIM_DATE'\n",
    "location_dim_table_name = 'DIM_LOCATION'\n",
    "\n",
    "# Create the dataset\n",
    "dataset_ref = client.dataset(dataset_name)\n",
    "client.get_dataset(dataset_ref)\n",
    "\n",
    "# Define the schema for the fact table\n",
    "fact_table_schema = [\n",
    "    bigquery.SchemaField('SALE_ID', 'STRING', mode='REQUIRED'),\n",
    "    bigquery.SchemaField('DATE_ID', 'INTEGER'),\n",
    "    bigquery.SchemaField('LOCATION_ID', 'INTEGER'),\n",
    "    bigquery.SchemaField('SALE_PRICE', 'INTEGER'),\n",
    "    bigquery.SchemaField('RESIDENTIAL_UNITS', 'INTEGER'),\n",
    "    bigquery.SchemaField('COMMERCIAL_UNITS', 'INTEGER'),\n",
    "    bigquery.SchemaField('TOTAL_UNITS', 'INTEGER'),\n",
    "    bigquery.SchemaField('LAND_SQFT', 'INTEGER'),\n",
    "    bigquery.SchemaField('GROSS_SQFT', 'INTEGER'),\n",
    "    bigquery.SchemaField('INITIAL_BUILDING_CLASS', 'STRING'),\n",
    "    bigquery.SchemaField('FINAL_BUILDING_CLASS', 'STRING'),\n",
    "    bigquery.SchemaField('INITIAL_TAX_CLASS', 'FLOAT'),\n",
    "    bigquery.SchemaField('FINAL_TAX_CLASS', 'FLOAT'),\n",
    "    bigquery.SchemaField('PROPERTIES_UNSOLD', 'BOOL'),\n",
    "    bigquery.SchemaField('PROPERTIES_UNSOLD_PRE_2020', 'BOOL'),\n",
    "    bigquery.SchemaField('PROPERTIES_UNSOLD_POST_2020', 'BOOL'),\n",
    "    bigquery.SchemaField('PROPERTIES_SOLD', 'BOOL'),\n",
    "    bigquery.SchemaField('PROPERTIES_SOLD_POST_2020', 'BOOL'),\n",
    "    bigquery.SchemaField('PROPERTIES_SOLD_PRE_2020', 'BOOL')\n",
    "]\n",
    "\n",
    "# Define the schema for the date dimension table\n",
    "date_dim_table_schema = [\n",
    "    bigquery.SchemaField('DATE_ID', 'INTEGER', mode='REQUIRED'),\n",
    "    bigquery.SchemaField('SALE_DATE', 'DATE'),\n",
    "    bigquery.SchemaField('YEAR_BUILT', 'INTEGER'),\n",
    "    bigquery.SchemaField('YEAR_SOLD', 'INTEGER'),\n",
    "    bigquery.SchemaField('MONTH_SOLD', 'INTEGER'),\n",
    "    bigquery.SchemaField('DAY_SOLD', 'INTEGER')\n",
    "]\n",
    "\n",
    "\n",
    "# Define the schema for the location dimension table\n",
    "location_dim_table_schema = [\n",
    "    bigquery.SchemaField('LOCATION_ID', 'INTEGER', mode='REQUIRED'),\n",
    "    bigquery.SchemaField('BIN', 'INTEGER'),\n",
    "    bigquery.SchemaField('ADDRESS', 'STRING'),\n",
    "    bigquery.SchemaField(\"BOROUGH\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"NEIGHBORHOOD\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"ZIP_CODE\", \"INTEGER\")\n",
    "]\n",
    "\n",
    "# Creates the SALES_FACT table:\n",
    "fact_table_ref = dataset_ref.table(fact_table_name)\n",
    "try:\n",
    "    client.get_table(fact_table_ref)\n",
    "    print(f\"Table {fact_table_name} already exists in the dataset {dataset_name}.\")\n",
    "except:\n",
    "    fact_table = bigquery.Table(fact_table_ref, schema=fact_table_schema)\n",
    "    client.create_table(fact_table)\n",
    "    print(f\"{fact_table_name} Created\")\n",
    "\n",
    "# Creates the DIM_DATE table:\n",
    "date_dim_table_ref = dataset_ref.table(date_dim_table_name)\n",
    "try:\n",
    "    client.get_table(date_dim_table_ref)\n",
    "    print(f\"Table {date_dim_table_name} already exists in the dataset {dataset_name}.\")\n",
    "except:\n",
    "    date_dim_table = bigquery.Table(date_dim_table_ref, schema=date_dim_table_schema)\n",
    "    client.create_table(date_dim_table)\n",
    "    print(f\"{date_dim_table_name} Created\")\n",
    "\n",
    "# Creates the DIM_LOCATION table:\n",
    "location_dim_table_ref = dataset_ref.table(location_dim_table_name)\n",
    "try:\n",
    "    client.get_table(location_dim_table_ref)\n",
    "    print(f\"Table {location_dim_table_name} already exists in the dataset {dataset_name}.\")\n",
    "except: \n",
    "    location_dim_table = bigquery.Table(location_dim_table_ref, schema=location_dim_table_schema)\n",
    "    client.create_table(location_dim_table)\n",
    "    print(f\"{location_dim_table_name} Created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e481a4",
   "metadata": {},
   "source": [
    "# Read a dataset from your Google Cloud Storage into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gcsfs import GCSFileSystem\n",
    "\n",
    "# Replace 'xx' with your actual initials\n",
    "gcs_bucket = 'cis-4400-project-xx'\n",
    "gcs_file_path = 'NYC_sales_cleaned.csv'\n",
    "\n",
    "# Use Pandas to read the dataset from GCS into a DataFrame\n",
    "df = pd.read_csv(f'gcs://{gcs_bucket}/{gcs_file_path}')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab34628",
   "metadata": {},
   "source": [
    "# Load Data into BigQuery Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4546ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a function that uploads your data to BigQuery from a DataFrame\n",
    "def upload_data_from_dataframe(df, table_ref):\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "    job_config.autodetect = True\n",
    "    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "    job.result()  # Wait for the job to complete\n",
    "\n",
    "# Split your DataFrame into the respective dimension and fact DataFrames\n",
    "# fact_df, date_dim_df, time_dim_df, location_dim_df = split_your_dataframe(df_transformed)\n",
    "def split_df(df):\n",
    "    fact_cols = [\n",
    "    \"SALE_ID\", \"DATE_ID\", \"LOCATION_ID\", \n",
    "    \"SALE_PRICE\", \"RESIDENTIAL_UNITS\", \n",
    "    \"COMMERCIAL_UNITS\", \"TOTAL_UNITS\", \n",
    "    \"LAND_SQFT\", \"GROSS_SQFT\", \n",
    "    \"INITIAL_BUILDING_CLASS\", \"FINAL_BUILDING_CLASS\", \n",
    "    \"INITIAL_TAX_CLASS\", \"FINAL_TAX_CLASS\", \n",
    "    \"PROPERTIES_UNSOLD\", \"PROPERTIES_UNSOLD_PRE_2020\", \n",
    "    \"PROPERTIES_UNSOLD_POST_2020\", \"PROPERTIES_SOLD\", \n",
    "    \"PROPERTIES_SOLD_POST_2020\", \"PROPERTIES_SOLD_PRE_2020\"]\n",
    "    \n",
    "    date_cols = [\n",
    "    \"DATE_ID\", \"SALE_DATE\", \"YEAR_BUILT\", \"YEAR_SOLD\", \"MONTH_SOLD\", \"DAY_SOLD\"]\n",
    "    \n",
    "\n",
    "    location_cols = [\n",
    "    \"LOCATION_ID\", \"BIN\", \"ADDRESS\", \"BOROUGH\", \n",
    "    \"NEIGHBORHOOD\", \"ZIP_CODE\"]\n",
    "\n",
    "    fact_df = df[fact_cols]\n",
    "    date_dim_df = df[date_cols]\n",
    "    location_dim_df = df[location_cols]\n",
    "    \n",
    "    # Return the split DataFrames\n",
    "    return fact_df, date_dim_df, location_dim_df\n",
    "\n",
    "fact_df, date_dim_df, location_dim_df = split_df(df)\n",
    "\n",
    "# Upload the data to BigQuery\n",
    "upload_data_from_dataframe(fact_df, fact_table_ref)\n",
    "upload_data_from_dataframe(date_dim_df, date_dim_table_ref)\n",
    "upload_data_from_dataframe(location_dim_df, location_dim_table_ref)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
