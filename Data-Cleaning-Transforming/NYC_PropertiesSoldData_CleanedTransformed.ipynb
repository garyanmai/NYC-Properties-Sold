{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3789f098",
   "metadata": {},
   "source": [
    "# Cleaning & Transformation Script for the NYC Citywide Annualized Calendar Sales Data\n",
    "\n",
    "**Link To Data**: https://data.cityofnewyork.us/City-Government/NYC-Citywide-Annualized-Calendar-Sales-Update/w2pb-icbu\n",
    "\n",
    "**API end-point**: https://data.cityofnewyork.us/resource/w2pb-icbu.json\n",
    "\n",
    "**Data Dictionary**: https://data.cityofnewyork.us/api/views/w2pb-icbu/files/8ed811b4-8238-4b5e-9acc-1e33d8705498?download=true&filename=Annualized_Calendar_Sales_Update%20Data_Dictionary.xlsx\n",
    "\n",
    "**Cleaned Data Dictionary**: https://docs.google.com/spreadsheets/d/17XyGmnw2fZuTMCWVKB1XiWGHQuwqWOidm0w80lbIyjE/edit?usp=sharing\n",
    "\n",
    "**IMPORTANT: This data set is 121.3 MB. Once downloaded, please keep the file in the same directory as this jupyter notebook file, so that the .csv file can be read into a pandas DataFrame correctly. After downloading the dataset, please rename it to 'NYC_sales.csv', to avoid any errors in reading the file**\n",
    "\n",
    "- First, we will read the data into a pandas DataFrame and start analyzing the data to gather insights on what should be cleaned\n",
    "- Afterwards, we can start rearranging columns, renaming as needed, and removing NULL/Duplicate values if necessary\n",
    "- After cleaning the data, we can then transform the data by creating new FACT columns generated. These columns can then be analyzed later for actionable insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f746ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reset warning filter to default (optional)\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reads CSV file into a pandas DataFrame:\n",
    "df = pd.read_csv('NYC_sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks the first five records of the DataFrame:\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b6e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks the number of rows and columns in the DataFrame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68363de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks the column names in the DataFrame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab4745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks the data types of each column in the DataFrame\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns from columns_to_drop list and into a new DataFrame called df_dropped\n",
    "# That way you do not overwrite the original DataFrame from above\n",
    "columns_to_drop = ['BUILDING CLASS CATEGORY','BLOCK','LOT','EASE-MENT','APARTMENT NUMBER','Longitude','Latitude','Community Board', 'Council District','Census Tract','BBL','Census Tract 2020','NTA','NTA Code']\n",
    "df_dropped = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display the new DataFrame after dropping columns\n",
    "print(\"\\nDataFrame after dropping columns:\")\n",
    "df_dropped.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns\n",
    "column_order = ['BIN','SALE DATE', 'SALE PRICE','ADDRESS', 'BOROUGH', 'NEIGHBORHOOD','ZIP CODE', 'RESIDENTIAL UNITS', 'COMMERCIAL UNITS', 'TOTAL UNITS', 'LAND SQUARE FEET', 'GROSS SQUARE FEET','BUILDING CLASS AT TIME OF SALE','BUILDING CLASS AS OF FINAL ROLL','TAX CLASS AT TIME OF SALE','TAX CLASS AS OF FINAL ROLL','YEAR BUILT']\n",
    "df_dropped = df_dropped[column_order]\n",
    "\n",
    "# Display the DataFrame after rearranging columns\n",
    "print(\"\\nDataFrame after rearranging columns:\")\n",
    "df_dropped.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30612557",
   "metadata": {},
   "source": [
    "### Now, let's count the number of Null/NaN records and Duplicate records. This will help us to understand areas we need to clean from our DataFrame. \n",
    "**For example, Since 'BIN' is our unique identifier, we will want to count the number of null/NaN values there in order to understand how many records we will need to drop later on in our transformation phase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e020291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count NaN records in column 'BIN'\n",
    "nan_count = df_dropped['BIN'].isna().sum()\n",
    "# Display the count of NaN records\n",
    "print(\"\\nNumber of NaN records in column 'BIN':\", nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9613ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count NaN records in each column of the DataFrame\n",
    "nan_counts = df_dropped.isna().sum()\n",
    "\n",
    "# Display the count of NaN records for each column of the DataFrame\n",
    "print(\"\\nNumber of NaN records in each column:\")\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfade5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count duplicate records in the entire DataFrame\n",
    "duplicate_count = df_dropped.duplicated().sum()\n",
    "\n",
    "# Display the count of duplicate records\n",
    "print(\"\\nNumber of duplicate records in the DataFrame:\", duplicate_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4104e62c",
   "metadata": {},
   "source": [
    "# Transformation Deliverables:\n",
    "**1. Unified date format YYYY-MM-DD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to a unified date format (YYYY-MM-DD)\n",
    "df_dropped['SALE DATE'] = pd.to_datetime(df_dropped['SALE DATE'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Display the DataFrame after converting the date format\n",
    "print(\"\\nDataFrame after converting the SALE DATE column to a unified date format (YYYY-MM-DD):\")\n",
    "df_dropped.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ea9d4",
   "metadata": {},
   "source": [
    "**2.  Splitting the date into multiple units (Year, Month, Day)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'SALE DATE' column to datetime\n",
    "df_dropped['SALE DATE'] = pd.to_datetime(df_dropped['SALE DATE'], errors='coerce')\n",
    "\n",
    "# Extract Year, Month, and Day into separate columns\n",
    "df_dropped['YEAR_SOLD'] = df_dropped['SALE DATE'].dt.year\n",
    "df_dropped['MONTH_SOLD'] = df_dropped['SALE DATE'].dt.month\n",
    "df_dropped['DAY_SOLD'] = df_dropped['SALE DATE'].dt.day\n",
    "\n",
    "# Display the DataFrame after splitting the date into multiple units\n",
    "print(\"\\nDataFrame after splitting the SALE DATE:\")\n",
    "df_dropped.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8440fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "new_column_names = {'SALE DATE': 'SALE_DATE', 'SALE PRICE': 'SALE_PRICE', 'ZIP CODE': 'ZIP_CODE', 'RESIDENTIAL UNITS': 'RESIDENTIAL_UNITS', 'COMMERCIAL UNITS': 'COMMERCIAL_UNITS', 'TOTAL UNITS': 'TOTAL_UNITS', 'LAND SQUARE FEET': 'LAND_SQFT', 'GROSS SQUARE FEET': 'GROSS_SQFT', 'BUILDING CLASS AT TIME OF SALE': 'INITIAL_BUILDING_CLASS', 'BUILDING CLASS AS OF FINAL ROLL': 'FINAL_BUILDING_CLASS', 'TAX CLASS AT TIME OF SALE': 'INITIAL_TAX_CLASS', 'TAX CLASS AS OF FINAL ROLL': 'FINAL_TAX_CLASS', 'YEAR BUILT':'YEAR_BUILT' }\n",
    "df_dropped.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "# Drop duplicate columns\n",
    "#columns_to_drop = ['YEAR','MONTH','DAY']\n",
    "#df_dropped = df_dropped.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display the DataFrame after renaming columns\n",
    "print(\"\\nDataFrame after renaming columns:\")\n",
    "df_dropped.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05e398",
   "metadata": {},
   "source": [
    "**3. Removing NULL/NaN values**\n",
    "- This step is important to do before we start changing data types\n",
    "- For example, if we want to change a column's data type from float to int, but there is a record with a null/NaN value, you will get an error: **IntCastingNaNError: Cannot convert non-finite values (NA or inf) to integer**\n",
    "- So, it's best to get rid of these null values first, to avoid any errors later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044bb3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows and columns\n",
    "df_dropped.shape\n",
    "\n",
    "#Drop null/NaN values from each row in the DataFrame that has atleast 1 null/NaN value\n",
    "df_cleaned = df_dropped.dropna()\n",
    "\n",
    "# Count the number of rows and columns after DataFrame has been cleaned\n",
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a53ea",
   "metadata": {},
   "source": [
    "**4. Removing Duplicate rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0143295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows, columns in the cleaned DataFrame\n",
    "df_cleaned.shape\n",
    "\n",
    "# Drop duplicate rows in the cleaned DataFrame\n",
    "df_cleaned.drop_duplicates(inplace=True)\n",
    "\n",
    "#Count the number of rows, columns in the cleaned DataFrame after dropping duplicates\n",
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb01e35",
   "metadata": {},
   "source": [
    "**5. Verify Data against data reference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf746786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert appropiate datatypes to int as necessary\n",
    "df_cleaned['BIN'] = df_cleaned['BIN'].astype(int)\n",
    "df_cleaned['ZIP_CODE'] = df_cleaned['ZIP_CODE'].astype(int)\n",
    "df_cleaned['RESIDENTIAL_UNITS'] = df_cleaned['RESIDENTIAL_UNITS'].astype(int)\n",
    "df_cleaned['COMMERCIAL_UNITS'] = df_cleaned['COMMERCIAL_UNITS'].astype(int)\n",
    "df_cleaned['TOTAL_UNITS'] = df_cleaned['TOTAL_UNITS'].astype(int)\n",
    "df_cleaned['YEAR_BUILT'] = df_cleaned['YEAR_BUILT'].astype(int)\n",
    "\n",
    "# Get rid of ',' and '-' in the 'LAND_SQFT' and 'GROSS_SQFT' columns\n",
    "df_cleaned['LAND_SQFT'] = df_cleaned['LAND_SQFT'].str.replace(',', '')\n",
    "df_cleaned['LAND_SQFT'] = df_cleaned['LAND_SQFT'].str.replace('-', '')\n",
    "df_cleaned['GROSS_SQFT'] = df_cleaned['GROSS_SQFT'].str.replace(',', '')\n",
    "df_cleaned['GROSS_SQFT'] = df_cleaned['GROSS_SQFT'].str.replace('-', '')\n",
    "\n",
    "# Convert the 'FINAL_TAX_CLASS' to numeric with errors='coerce'\n",
    "# errors='coerce' converts the column to numeric values, otherwise non-convertible values are replaced with NaN\n",
    "df_cleaned['FINAL_TAX_CLASS'] = pd.to_numeric(df_cleaned['FINAL_TAX_CLASS'], errors='coerce')\n",
    "\n",
    "# Convert the remaining column to numeric\n",
    "df_cleaned['LAND_SQFT'] = pd.to_numeric(df_cleaned['LAND_SQFT'])\n",
    "df_cleaned['GROSS_SQFT'] = pd.to_numeric(df_cleaned['GROSS_SQFT'])\n",
    "df_cleaned['FINAL_TAX_CLASS'] = pd.to_numeric(df_cleaned['FINAL_TAX_CLASS'])\n",
    "\n",
    "# Convert 'INITIAL_TAX_CLASS' column into float data type to match FINAL_TAX_CLASS column data type\n",
    "df_cleaned['INITIAL_TAX_CLASS'] = df_cleaned['INITIAL_TAX_CLASS'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display DataFrame with edited data types\n",
    "df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3dc645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if datatypes have been converted sucessfully\n",
    "df_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ef320",
   "metadata": {},
   "source": [
    "**Here, we are cleaning the data further. Since, the 'BIN' column has duplicate records, we must remove those records since 'BIN' is our unique identifier column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fe3d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicate values in the 'BIN' column\n",
    "duplicate_count = df_cleaned.duplicated(subset=['BIN']).sum()\n",
    "\n",
    "# Display the count of duplicate values\n",
    "print(\"\\nNumber of duplicate values in 'BIN':\", duplicate_count)\n",
    "\n",
    "# Drop records with the same value in 'BIN'\n",
    "# keep='first' will keep the first record and delete it's duplicates\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['BIN'], keep='first')\n",
    "\n",
    "# Count the number of duplicate values in the 'BIN' column again\n",
    "duplicate_count = df_cleaned.duplicated(subset=['BIN']).sum()\n",
    "\n",
    "# Display the count of duplicate values\n",
    "print(\"\\nNumber of duplicate values in 'BIN' after cleaning:\", duplicate_count)\n",
    "\n",
    "# See the new shape of the cleaned DataFrame\n",
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c671b38",
   "metadata": {},
   "source": [
    "**Now, you should have a near-fully cleaned data set based on your requirements. We can then move on to using our FACT columns to create aggregable columns that can provide us with actionable insights**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6425023",
   "metadata": {},
   "source": [
    "**6. Adding one or many columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column(s) for properties that have not been sold\n",
    "df_cleaned['PROPERTIES_UNSOLD'] = df_cleaned['SALE_PRICE'] <= 0\n",
    "df_cleaned['PROPERTIES_UNSOLD_PRE_2020'] = (df_cleaned['YEAR_SOLD'].isin([2017, 2018, 2019])) & (df_cleaned['SALE_PRICE'] <= 0)\n",
    "df_cleaned['PROPERTIES_UNSOLD_POST_2020'] = (df_cleaned['YEAR_SOLD'].isin([2020, 2021, 2022])) & (df_cleaned['SALE_PRICE'] <= 0)\n",
    "\n",
    "# Add column(s) for properties that have been sold\n",
    "df_cleaned['PROPERTIES_SOLD_POST_2020'] = (df_cleaned['YEAR_SOLD'].isin([2020, 2021, 2022])) & (df_cleaned['SALE_PRICE'] > 0)\n",
    "df_cleaned['PROPERTIES_SOLD_PRE_2020'] = (df_cleaned['YEAR_SOLD'].isin([2017, 2018, 2019])) & (df_cleaned['SALE_PRICE'] > 0)\n",
    "\n",
    "# Show DataFrame with new column\n",
    "df_cleaned.head(5)\n",
    "\n",
    "# Count Number of properties not yet sold\n",
    "count_properties_unsold = df_cleaned['PROPERTIES_UNSOLD'].sum()\n",
    "print(f'Number of properties not yet sold: {count_properties_unsold}')\n",
    "\n",
    "# Count Number of properties not yet sold pre-pandemic\n",
    "count_properties_unsold_pre_2020 = df_cleaned['PROPERTIES_UNSOLD_PRE_2020'].sum()\n",
    "print(f'Number of properties not yet sold pre-pandemic: {count_properties_unsold_pre_2020}')\n",
    "\n",
    "# Count Number of properties not yet sold post-pandemic\n",
    "count_properties_unsold_post_2020 = df_cleaned['PROPERTIES_UNSOLD_POST_2020'].sum()\n",
    "print(f'Number of properties not yet sold post-pandemic: {count_properties_unsold_post_2020}')\n",
    "\n",
    "# Count Number of properties sold post-pandemic\n",
    "count_properties_sold_post_2020 = df_cleaned['PROPERTIES_SOLD_POST_2020'].sum()\n",
    "print(f'\\nNumber of properties sold post-pandemic: {count_properties_sold_post_2020}')\n",
    "\n",
    "# Count Number of properties sold pre-pandemic\n",
    "count_properties_sold_pre_2020 = df_cleaned['PROPERTIES_SOLD_PRE_2020'].sum()\n",
    "print(f'Number of properties sold pre-pandemic: {count_properties_sold_pre_2020}')\n",
    "\n",
    "# Find the difference of properties un-sold pre and post pandemic\n",
    "difference_properties_unsold_preandpost_2020 = count_properties_unsold_post_2020 - count_properties_unsold_pre_2020\n",
    "print(f'\\nDifference in properties unsold pre and post pandemic: {difference_properties_unsold_preandpost_2020} more properties unsold post-pandemic')\n",
    "print('Conclusion: There are more properties unsold post-pandemic.')\n",
    "\n",
    "# Find the difference of properties sold pre and post pandemic\n",
    "difference_properties_sold_preandpost_2020 = count_properties_sold_pre_2020 - count_properties_sold_post_2020\n",
    "print(f'\\nDifference in properties sold pre and post pandemic: {difference_properties_sold_preandpost_2020} more properties sold pre-pandemic')\n",
    "print('Conclusion: There are more properties sold pre-pandemic.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d66c151",
   "metadata": {},
   "source": [
    "**Below, you can run the following codes to find unique values in columns. You can then gather more insights with those unique values.**\n",
    "\n",
    "**Some ideas are:**\n",
    "- Find the years with the most sold properties\n",
    "- Find the months with the most sold properties\n",
    "- Find the days with the most sold properties\n",
    "- Find the average number of sales in each month or year\n",
    "- Average number of residential units in properties sold\n",
    "\n",
    "**You can also find which months had more active sales pre and post 2020 to see how the pandemic may have affected sales. Please note that the pandemic started in March 2020. The months before may or may not reflect the effects of the pandemic yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique values in columns\n",
    "unique_years = df_cleaned['YEAR_SOLD'].unique()\n",
    "unique_years = sorted(unique_years)\n",
    "\n",
    "unique_months = df_cleaned['MONTH_SOLD'].unique()\n",
    "unique_months = sorted(unique_months)\n",
    "\n",
    "unique_days = df_cleaned['DAY_SOLD'].unique()\n",
    "unique_days = sorted(unique_days)\n",
    "\n",
    "print(unique_years)\n",
    "print(unique_months)\n",
    "print(unique_days)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
